{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Supporting Reproducible IR Experiments with Metadata Annotations of Open Runs Experimentation in information retrieval (IR) research is an inherently data-driven process that often results in experimental artifacts - so called run files. In order to promote the reproduciblity of IR experiments, Voorhees et al. introduced the idea of Open Runs proposing to provide every run file with an open-source software repository. We build up on the idea of Open runs and propose to make the experimental artifacts even more valuable and reproducible by metadata annotations of run files. We align the metadata schema to the PRIMAD model which provides a conceptual taxonomy for reproducible IR experiments. From a practical point of view, we propose to add the metadata, similar to a file header, as comments in the beginning of the run file. The commonly used evaluation toolkit trec_eval allows to add comments in the run files by starting line comments with # and an official support is in development for v10.0 . This website hosts an introduction of the outlined metadata schema of Open Runs for which more details and background information can be found in our SIGIR resource paper. For each PRIMAD component this website provides checklists that can be used as a reference when annotating run files in order to prepare them for reproducibility. Besides this website, we give an introduction to the metadata and the software support of repro_eval in a Colab notebook that makes use of some annotated runs that are taken from our curated dataset with annotated runs hosted in a Zenodo archive.","title":"Metadata Annotations of Open Runs"},{"location":"#supporting-reproducible-ir-experiments-with-metadata-annotations-of-open-runs","text":"Experimentation in information retrieval (IR) research is an inherently data-driven process that often results in experimental artifacts - so called run files. In order to promote the reproduciblity of IR experiments, Voorhees et al. introduced the idea of Open Runs proposing to provide every run file with an open-source software repository. We build up on the idea of Open runs and propose to make the experimental artifacts even more valuable and reproducible by metadata annotations of run files. We align the metadata schema to the PRIMAD model which provides a conceptual taxonomy for reproducible IR experiments. From a practical point of view, we propose to add the metadata, similar to a file header, as comments in the beginning of the run file. The commonly used evaluation toolkit trec_eval allows to add comments in the run files by starting line comments with # and an official support is in development for v10.0 . This website hosts an introduction of the outlined metadata schema of Open Runs for which more details and background information can be found in our SIGIR resource paper. For each PRIMAD component this website provides checklists that can be used as a reference when annotating run files in order to prepare them for reproducibility. Besides this website, we give an introduction to the metadata and the software support of repro_eval in a Colab notebook that makes use of some annotated runs that are taken from our curated dataset with annotated runs hosted in a Zenodo archive.","title":"Supporting Reproducible IR Experiments with Metadata Annotations of Open Runs"},{"location":"colab/","text":"Demonstration on Colab In order to demonstrate what kinds of reproducibility experiments and meta-evaluation are made possible through the metadata annotations, we provide an interactive Python notebook that can be run on Google Colab . The notebook gives a general introduction to the metadata handling that is implemented by the metadata -module in repro_eval and provides the experiments of Section 6 in the SIGIR resource paper.","title":"Colab notebook"},{"location":"colab/#demonstration-on-colab","text":"In order to demonstrate what kinds of reproducibility experiments and meta-evaluation are made possible through the metadata annotations, we provide an interactive Python notebook that can be run on Google Colab . The notebook gives a general introduction to the metadata handling that is implemented by the metadata -module in repro_eval and provides the experiments of Section 6 in the SIGIR resource paper.","title":"Demonstration on Colab"},{"location":"dataset/","text":"Annotated dataset We have curated a dataset of annotated run files resulting from different experiments that share the same retrieval method on a more abstract level. All of these runs are based on cross-collection relevance feedback for which relevance labels and the corresponding documents from one or more source collections are used as training data to train a relevance classifier that ranks documents of a target collection . While some of the runs were available from the TREC run archive, others were reimplemented by us. All of the runs are annotated in accordance with the outlined metadata schema. The dataset is hosted in an external Zenodo archive . Some of the runs are used for the demonstration on Colab. The run dataset is compiled from the following reproduced experiments: Grossman and Cormack @ TREC Common Core 2017 Paper | Runs Grossman and Cormack @ TREC Common Core 2018 Paper | Runs Yu et al. @ TREC Common Core 2018 Paper | Runs Yu et al. @ ECIR 2019 Paper | Runs Breuer et al. @ SIGIR 2020 Paper | Runs Breuer et al. @ CLEF 2021 Paper | Runs The figure below illustrates the principle idea behind cross-collection relevance feedback.","title":"Dataset"},{"location":"dataset/#annotated-dataset","text":"We have curated a dataset of annotated run files resulting from different experiments that share the same retrieval method on a more abstract level. All of these runs are based on cross-collection relevance feedback for which relevance labels and the corresponding documents from one or more source collections are used as training data to train a relevance classifier that ranks documents of a target collection . While some of the runs were available from the TREC run archive, others were reimplemented by us. All of the runs are annotated in accordance with the outlined metadata schema. The dataset is hosted in an external Zenodo archive . Some of the runs are used for the demonstration on Colab. The run dataset is compiled from the following reproduced experiments: Grossman and Cormack @ TREC Common Core 2017 Paper | Runs Grossman and Cormack @ TREC Common Core 2018 Paper | Runs Yu et al. @ TREC Common Core 2018 Paper | Runs Yu et al. @ ECIR 2019 Paper | Runs Breuer et al. @ SIGIR 2020 Paper | Runs Breuer et al. @ CLEF 2021 Paper | Runs The figure below illustrates the principle idea behind cross-collection relevance feedback.","title":"Annotated dataset"},{"location":"software/","text":"Metadata module of repro_eval In order to provide a software interface for developers and experimenters, we have implemented the metadata support into repro_eval - the reproducibility toolkit for system-oriented IR experiments. In the following we provide a brief overview of the metadata -module and its classes, for more details we refer the interested reader to the actual implementations in the GitHub repository. MetadataHandler The MetadataHandler is used for in- and output operations of annotated run files, e.g., it can be used to annotate run files with a template in the form of a YAML file. from repro_eval.metadata import MetadataHandler metadata_handler = MetadataHandler(run_path='./run.txt', metadata_path='./metadata.yaml') metadata_handler.write_metadata() MetadataAnalyzer The MetadataAnalyzer is used to analyze sets of different run files in reference to a run that has be be provided upon instantiation. The analyze_directory() method returns a dictionary with PRIMAD identifiers as keys and lists with the corresponding run paths as values. from repro_eval.metadata import MetadataAnalyzer metadata_analyzer = MetadataAnalyzer(run_path='./run.txt') experiments = metadata_analyzer.analyze_directory(dir_path='./runs/') ... PrimadExperiment The PrimadExperiment is used to determine the reproducibility measures between a reference run and a set of one or more reproduced run files. Depending on the type of the PRIMAD experiment, several reproducibility measures can be determined. We differentiate between different PRIMAD types by lower- and upper-case letters in the identifier. For instance, if the Method is changed by parameter sweeps, the corresponding identifier results as priMad with an upper-case M. ... from repro_eval.metadata import PrimadExperiment run_candidates = experiments.get('priMad') primad_experiment = PrimadExperiment(primad='priMad', ref_base_path='./run.txt', rep_base=run_candidates, rpd_qrels='./qrels.txt') primad_experiment.evaluate()","title":"Software support"},{"location":"software/#metadata-module-of-repro_eval","text":"In order to provide a software interface for developers and experimenters, we have implemented the metadata support into repro_eval - the reproducibility toolkit for system-oriented IR experiments. In the following we provide a brief overview of the metadata -module and its classes, for more details we refer the interested reader to the actual implementations in the GitHub repository.","title":"Metadata module of repro_eval"},{"location":"software/#metadatahandler","text":"The MetadataHandler is used for in- and output operations of annotated run files, e.g., it can be used to annotate run files with a template in the form of a YAML file. from repro_eval.metadata import MetadataHandler metadata_handler = MetadataHandler(run_path='./run.txt', metadata_path='./metadata.yaml') metadata_handler.write_metadata()","title":"MetadataHandler"},{"location":"software/#metadataanalyzer","text":"The MetadataAnalyzer is used to analyze sets of different run files in reference to a run that has be be provided upon instantiation. The analyze_directory() method returns a dictionary with PRIMAD identifiers as keys and lists with the corresponding run paths as values. from repro_eval.metadata import MetadataAnalyzer metadata_analyzer = MetadataAnalyzer(run_path='./run.txt') experiments = metadata_analyzer.analyze_directory(dir_path='./runs/') ...","title":"MetadataAnalyzer"},{"location":"software/#primadexperiment","text":"The PrimadExperiment is used to determine the reproducibility measures between a reference run and a set of one or more reproduced run files. Depending on the type of the PRIMAD experiment, several reproducibility measures can be determined. We differentiate between different PRIMAD types by lower- and upper-case letters in the identifier. For instance, if the Method is changed by parameter sweeps, the corresponding identifier results as priMad with an upper-case M. ... from repro_eval.metadata import PrimadExperiment run_candidates = experiments.get('priMad') primad_experiment = PrimadExperiment(primad='priMad', ref_base_path='./run.txt', rep_base=run_candidates, rpd_qrels='./qrels.txt') primad_experiment.evaluate()","title":"PrimadExperiment"},{"location":"metadata/actor/","text":"Actor The metadata information about the Actor should include all available public information about the experimenter who actually executed the software of the experiments. Often this person is hidden behind the co-authorship and the metadata should report how to reach the experimenter for further questions. The role identifies if the author is the original experimenter or the reproducer of a run/experiment. Checklist actor \u2192 name Description: Name of the Actor. Type: Scalar ; usually a string of characters actor \u2192 orcid Description: ORCID of the Actor. Type: Scalar ; usually a string of characters actor \u2192 team Description: Team name of which the Actor is part of. Type: Scalar ; usually a string of characters actor \u2192 fields Description: List of the Actor's research fields. Type: Sequence of scalars ; usually a listing of strings actor \u2192 mail Description: Mail address of the Actor. Type: Scalar ; usually a string of characters actor \u2192 role Description: Role of the Actor. Can be experimenter if the original experiment is conducted or reproducer if the experiment is reproduced. Type: Scalar ; usually a string of characters actor \u2192 degree Description: The academic degree of the Actor, e.g., B.Sc. , M.Sc. , or Ph.D. Type: Scalar ; usually a string of characters actor \u2192 github Description: GitHub handle of the Actor. Type: Scalar ; usually a string of characters actor \u2192 twitter Description: Twitter handle of the Actor. Type: Scalar ; usually a string of characters Example actor: name: Jimmy Lin orcid: 0000-0002-0661-7189 team: h2oloo fields: - nlp - ir - databases - large-scale distributed algorithms - data analytics mail: jimmylin@uwaterloo.ca role: reproducer degree: Ph.D. github: https://github.com/lintool twitter: https://twitter.com/lintool","title":"Actor"},{"location":"metadata/actor/#actor","text":"The metadata information about the Actor should include all available public information about the experimenter who actually executed the software of the experiments. Often this person is hidden behind the co-authorship and the metadata should report how to reach the experimenter for further questions. The role identifies if the author is the original experimenter or the reproducer of a run/experiment.","title":"Actor"},{"location":"metadata/actor/#checklist","text":"actor \u2192 name Description: Name of the Actor. Type: Scalar ; usually a string of characters actor \u2192 orcid Description: ORCID of the Actor. Type: Scalar ; usually a string of characters actor \u2192 team Description: Team name of which the Actor is part of. Type: Scalar ; usually a string of characters actor \u2192 fields Description: List of the Actor's research fields. Type: Sequence of scalars ; usually a listing of strings actor \u2192 mail Description: Mail address of the Actor. Type: Scalar ; usually a string of characters actor \u2192 role Description: Role of the Actor. Can be experimenter if the original experiment is conducted or reproducer if the experiment is reproduced. Type: Scalar ; usually a string of characters actor \u2192 degree Description: The academic degree of the Actor, e.g., B.Sc. , M.Sc. , or Ph.D. Type: Scalar ; usually a string of characters actor \u2192 github Description: GitHub handle of the Actor. Type: Scalar ; usually a string of characters actor \u2192 twitter Description: Twitter handle of the Actor. Type: Scalar ; usually a string of characters","title":"Checklist"},{"location":"metadata/actor/#example","text":"actor: name: Jimmy Lin orcid: 0000-0002-0661-7189 team: h2oloo fields: - nlp - ir - databases - large-scale distributed algorithms - data analytics mail: jimmylin@uwaterloo.ca role: reproducer degree: Ph.D. github: https://github.com/lintool twitter: https://twitter.com/lintool","title":"Example"},{"location":"metadata/data/","text":"Data The metadata information about the Data should include the test collection, training data and others. For the test collection, the data source as well as the location of the qrels and topics files have to be reported. If available, the identifier that is chosen by the data catalog ir_datasets should be reported as well. The example below uses another test collection as training data, but generally, this entry should be reported as a list, especially if more than one data source is used in the experiments. The third subcomponent other covers miscellaneous information related to the data, for instance, about word embeddings or stopwords. Checklist data \u2192 test collection Description: A test collection includes but is not limited to the following components: data \u2192 test collection \u2192 name : Name of the test collection. data \u2192 test collection \u2192 source : Official source of the collection. data \u2192 test collection \u2192 qrels : Source of the qrels. data \u2192 test collection \u2192 topics : Source of the topic file. data \u2192 test collection \u2192 ir_datasets : Identifier in ir_datasets . Type: Collection of scalars data \u2192 training data Description: List of different training data sources that are used in the experiments. data \u2192 training data \u2192 name data \u2192 training data \u2192 source Type: Sequence of mappings ; a mapping usually has a name and a source data \u2192 other Description: List of other data sources that are used in the experiments, for instance, external stopword lists, thesauri, or word embeddings. data \u2192 other \u2192 name data \u2192 other \u2192 source Type: Sequence of mappings ; a mapping usually has a name and a source Example data: test collection: name: The New York Times Annotated Corpus source: https://catalog.ldc.upenn.edu/LDC2008T19 qrels: https://trec.nist.gov/data/core/qrels.txt topics: https://trec.nist.gov/data/core/core_nist.txt ir_datasets: https://ir-datasets.com/nyt training data: - name: TREC disks 4 and 5 source: https://trec.nist.gov/data/cd45/index.html qrels: https://trec.nist.gov/data/robust/qrels.robust2004.txt topics: https://trec.nist.gov/data/robust/04.testset.gz ir_datasets: https://ir-datasets.com/trec-robust04 other: - name: GloVe embeddings source: https://nlp.stanford.edu/projects/glove/ - name: Indri's stopword list source: https://sourceforge.net/projects/lemur/","title":"Data"},{"location":"metadata/data/#data","text":"The metadata information about the Data should include the test collection, training data and others. For the test collection, the data source as well as the location of the qrels and topics files have to be reported. If available, the identifier that is chosen by the data catalog ir_datasets should be reported as well. The example below uses another test collection as training data, but generally, this entry should be reported as a list, especially if more than one data source is used in the experiments. The third subcomponent other covers miscellaneous information related to the data, for instance, about word embeddings or stopwords.","title":"Data"},{"location":"metadata/data/#checklist","text":"data \u2192 test collection Description: A test collection includes but is not limited to the following components: data \u2192 test collection \u2192 name : Name of the test collection. data \u2192 test collection \u2192 source : Official source of the collection. data \u2192 test collection \u2192 qrels : Source of the qrels. data \u2192 test collection \u2192 topics : Source of the topic file. data \u2192 test collection \u2192 ir_datasets : Identifier in ir_datasets . Type: Collection of scalars data \u2192 training data Description: List of different training data sources that are used in the experiments. data \u2192 training data \u2192 name data \u2192 training data \u2192 source Type: Sequence of mappings ; a mapping usually has a name and a source data \u2192 other Description: List of other data sources that are used in the experiments, for instance, external stopword lists, thesauri, or word embeddings. data \u2192 other \u2192 name data \u2192 other \u2192 source Type: Sequence of mappings ; a mapping usually has a name and a source","title":"Checklist"},{"location":"metadata/data/#example","text":"data: test collection: name: The New York Times Annotated Corpus source: https://catalog.ldc.upenn.edu/LDC2008T19 qrels: https://trec.nist.gov/data/core/qrels.txt topics: https://trec.nist.gov/data/core/core_nist.txt ir_datasets: https://ir-datasets.com/nyt training data: - name: TREC disks 4 and 5 source: https://trec.nist.gov/data/cd45/index.html qrels: https://trec.nist.gov/data/robust/qrels.robust2004.txt topics: https://trec.nist.gov/data/robust/04.testset.gz ir_datasets: https://ir-datasets.com/trec-robust04 other: - name: GloVe embeddings source: https://nlp.stanford.edu/projects/glove/ - name: Indri's stopword list source: https://sourceforge.net/projects/lemur/","title":"Example"},{"location":"metadata/implementation/","text":"Implementation The metadata information about the Implementation should include the URL of the open-source repository if available. If so, also the commit at which the code of the repository was used should be made explicit. If the experiments are run from the command line, the corresponding commands including all the arguments and parameters should be added. Checklist implementation \u2192 executable \u2192 cmd Description: The software command that was used to conduct the experiments, i.e., to make the run. Type: Scalar ; usually a string of characters implementation \u2192 source \u2192 lang Description: All programming languages that were used for the experiments. Type: Sequence of scalars ; listing of strings implementation \u2192 source \u2192 repository Description: The URL of the corresponding software repository. Type: Scalar ; usually a string of characters implementation \u2192 source \u2192 commit Description: The commit at which the repository was used for the experiments. Type: Scalar ; usually a string of characters Example implementation: executable: cmd: python src/main/python/ecir2019_ccrf/generate_runs.py --config src/main/python/ecir2019_ccrf/configs/ccrf.04_core17_BM25+AX.json source: lang: - python repository: https://github.com/castorini/anserini commit: 9548cd6","title":"Implementation"},{"location":"metadata/implementation/#implementation","text":"The metadata information about the Implementation should include the URL of the open-source repository if available. If so, also the commit at which the code of the repository was used should be made explicit. If the experiments are run from the command line, the corresponding commands including all the arguments and parameters should be added.","title":"Implementation"},{"location":"metadata/implementation/#checklist","text":"implementation \u2192 executable \u2192 cmd Description: The software command that was used to conduct the experiments, i.e., to make the run. Type: Scalar ; usually a string of characters implementation \u2192 source \u2192 lang Description: All programming languages that were used for the experiments. Type: Sequence of scalars ; listing of strings implementation \u2192 source \u2192 repository Description: The URL of the corresponding software repository. Type: Scalar ; usually a string of characters implementation \u2192 source \u2192 commit Description: The commit at which the repository was used for the experiments. Type: Scalar ; usually a string of characters","title":"Checklist"},{"location":"metadata/implementation/#example","text":"implementation: executable: cmd: python src/main/python/ecir2019_ccrf/generate_runs.py --config src/main/python/ecir2019_ccrf/configs/ccrf.04_core17_BM25+AX.json source: lang: - python repository: https://github.com/castorini/anserini commit: 9548cd6","title":"Example"},{"location":"metadata/method/","text":"Method The metadata information about the Method should include if the run is automatic or manual, i.e., derived with or without a human-in-the-loop approach, and information regarding the indexing and retrieval. While the indexing is usually composed of several processing steps including tokenization, stemming, and stopword removal, the retrieval method assigns scores between a query and a document. Modern retrieval pipelines are often realized in the form of a multi-stage ranking that is supported by our metadata schema as well. If an additional ranking method reranks the output of a previous retrieval method, it can be reported by the reranks entry that refers to the name of the previous method. Likewise, it is possible to report interpolated scores. Checklist method \u2192 automatic Description: Boolean value indicating if it is a automatic ( true ) or manual ( false ) run. Type: Boolean ; true if automatic, false if manual method \u2192 score ties Description: Name of the method used to break score ties in the ranking. Type: Scalar ; usually a string of characters method \u2192 indexing \u2192 tokenizer Description: Name of the tokenizer. Type: Scalar ; usually a string of characters method \u2192 indexing \u2192 stemmer Description: Name of the stemmer. Type: Scalar ; usually a string of characters method \u2192 indexing \u2192 stopwords Description: Name of the stopword list. Type: Scalar ; usually a string of characters method \u2192 retrieval Description: Each mapping represents one component of a ranking pipeline, i.e., it is also possible to report multi-stage ranking pipelines by referring to previous ranking stages. The following scalars should be reported if required: method \u2192 retrieval \u2192 name : Name of the ranking stage component. method \u2192 retrieval \u2192 method : Name of the actual retrieval method. method \u2192 retrieval \u2192 params : Parameters of the retrieval method. method \u2192 retrieval \u2192 reranks : Name of the component whose output will be reranked. method \u2192 retrieval \u2192 interpolates : List of components whose output is interpolated. method \u2192 retrieval \u2192 weight : Interpolation weight. Type: Sequence of mappings ; one mapping represents one component in a multi-stage ranking pipeline Example method: automatic: true indexing: tokenizer: org.apache.lucene.analysis.en.StandardTokenizer stemmer: org.apache.lucene.analysis.en.PorterStemFilter stopwords: org.apache.lucene.analysis.standard.StandardAnalyzer.STOP_WORDS_SET retrieval: - name: bm25 method: org.apache.lucene.search.similarities.Similarity.BM25Similarity b: 0.4 k1: 0.9 - name: axiomatic reranker method: io.anserini.rerank.lib.AxiomReranker rerankCutoff: 20 axiom.deterministic: true reranks: bm25 - name: lr reranker method: sklearn.linear_model.LogisticRegression reranks: axiomatic reranker - name: svm reranker method: sklearn.svm.SVC reranks: axiomatic reranker - name: lgb reranker method: lightgbm reranks: axiomatic reranker - name: ensemble ensembles: - lr reranker - svm reranker - lgb reranker - name: interpolation weight: 0.6 interpolates: - axiomatic reranker - ensemble","title":"Method"},{"location":"metadata/method/#method","text":"The metadata information about the Method should include if the run is automatic or manual, i.e., derived with or without a human-in-the-loop approach, and information regarding the indexing and retrieval. While the indexing is usually composed of several processing steps including tokenization, stemming, and stopword removal, the retrieval method assigns scores between a query and a document. Modern retrieval pipelines are often realized in the form of a multi-stage ranking that is supported by our metadata schema as well. If an additional ranking method reranks the output of a previous retrieval method, it can be reported by the reranks entry that refers to the name of the previous method. Likewise, it is possible to report interpolated scores.","title":"Method"},{"location":"metadata/method/#checklist","text":"method \u2192 automatic Description: Boolean value indicating if it is a automatic ( true ) or manual ( false ) run. Type: Boolean ; true if automatic, false if manual method \u2192 score ties Description: Name of the method used to break score ties in the ranking. Type: Scalar ; usually a string of characters method \u2192 indexing \u2192 tokenizer Description: Name of the tokenizer. Type: Scalar ; usually a string of characters method \u2192 indexing \u2192 stemmer Description: Name of the stemmer. Type: Scalar ; usually a string of characters method \u2192 indexing \u2192 stopwords Description: Name of the stopword list. Type: Scalar ; usually a string of characters method \u2192 retrieval Description: Each mapping represents one component of a ranking pipeline, i.e., it is also possible to report multi-stage ranking pipelines by referring to previous ranking stages. The following scalars should be reported if required: method \u2192 retrieval \u2192 name : Name of the ranking stage component. method \u2192 retrieval \u2192 method : Name of the actual retrieval method. method \u2192 retrieval \u2192 params : Parameters of the retrieval method. method \u2192 retrieval \u2192 reranks : Name of the component whose output will be reranked. method \u2192 retrieval \u2192 interpolates : List of components whose output is interpolated. method \u2192 retrieval \u2192 weight : Interpolation weight. Type: Sequence of mappings ; one mapping represents one component in a multi-stage ranking pipeline","title":"Checklist"},{"location":"metadata/method/#example","text":"method: automatic: true indexing: tokenizer: org.apache.lucene.analysis.en.StandardTokenizer stemmer: org.apache.lucene.analysis.en.PorterStemFilter stopwords: org.apache.lucene.analysis.standard.StandardAnalyzer.STOP_WORDS_SET retrieval: - name: bm25 method: org.apache.lucene.search.similarities.Similarity.BM25Similarity b: 0.4 k1: 0.9 - name: axiomatic reranker method: io.anserini.rerank.lib.AxiomReranker rerankCutoff: 20 axiom.deterministic: true reranks: bm25 - name: lr reranker method: sklearn.linear_model.LogisticRegression reranks: axiomatic reranker - name: svm reranker method: sklearn.svm.SVC reranks: axiomatic reranker - name: lgb reranker method: lightgbm reranks: axiomatic reranker - name: ensemble ensembles: - lr reranker - svm reranker - lgb reranker - name: interpolation weight: 0.6 interpolates: - axiomatic reranker - ensemble","title":"Example"},{"location":"metadata/overview/","text":"Metadata schema In the following, we provide an overview of the metadata, but we also refer the interested reader to our SIGIR resource paper where each (sub)component is backed with studies from the literature. Our metadata schema is based on PRIMAD which stands for P latform, R esearch goal, I mplementation, M ethod, A ctor, D ata. Each of these components is essential for reproducible IR experimentation. We align the metadata information to the PRIMAD model and propose subcomponents that should be added to each metadata entry if required. Besides a short description of each PRIMAD component, we provide a checklist that can be used as a reference when annotating runs and we provide illustratives examples. The current schema-version is v0.1 . General information The metadata annotations should start and end with the ir_metadata identifier, i.e., ir_metadata.start in the first line and ir_metadata.end in the last line of the metadata header. In general, it has to follow the YAML formatting conventions and in the checklists we stick to the YAML terminology when describing the subcomponent types. At the beginning, the metadata includes some general information about the current schema-version (this version), the version of the run metadata (in case the metadata is updated in the future), and the run tag that usually can be found in the last column of a TREC run. This more general information is followed by the metadata for each PRIMAD component for which we provide checklists and examples on the following sites. ir_metadata.start schema-version: 0.1 run-version: 1.0 tag: h2oloo.ccrf.04.core17.ax_e3_0.6 platform: ... research goal: ... implementation: ... method: ... actor: ... data: ... ir_metadata.end","title":"Overview"},{"location":"metadata/overview/#metadata-schema","text":"In the following, we provide an overview of the metadata, but we also refer the interested reader to our SIGIR resource paper where each (sub)component is backed with studies from the literature. Our metadata schema is based on PRIMAD which stands for P latform, R esearch goal, I mplementation, M ethod, A ctor, D ata. Each of these components is essential for reproducible IR experimentation. We align the metadata information to the PRIMAD model and propose subcomponents that should be added to each metadata entry if required. Besides a short description of each PRIMAD component, we provide a checklist that can be used as a reference when annotating runs and we provide illustratives examples. The current schema-version is v0.1 .","title":"Metadata schema"},{"location":"metadata/overview/#general-information","text":"The metadata annotations should start and end with the ir_metadata identifier, i.e., ir_metadata.start in the first line and ir_metadata.end in the last line of the metadata header. In general, it has to follow the YAML formatting conventions and in the checklists we stick to the YAML terminology when describing the subcomponent types. At the beginning, the metadata includes some general information about the current schema-version (this version), the version of the run metadata (in case the metadata is updated in the future), and the run tag that usually can be found in the last column of a TREC run. This more general information is followed by the metadata for each PRIMAD component for which we provide checklists and examples on the following sites. ir_metadata.start schema-version: 0.1 run-version: 1.0 tag: h2oloo.ccrf.04.core17.ax_e3_0.6 platform: ... research goal: ... implementation: ... method: ... actor: ... data: ... ir_metadata.end","title":"General information"},{"location":"metadata/platform/","text":"Platform The metadata information about the Platform should include the underlying hardware, the operating system, and the used software libraries. software covers every package or library that is used for the Implementation . If a retrieval toolkit is used, it should be reported explicitly. Checklist platform \u2192 hardware \u2192 cpu \u2192 model Description: Name of the CPU model. Type: Scalar ; usually a string of characters platform \u2192 hardware \u2192 cpu \u2192 architecture Description: Identifier of the CPU architecture. Type: Scalar ; usually a string of characters platform \u2192 hardware \u2192 cpu \u2192 operation mode Description: Operation mode of the CPU. Type: Scalar ; usually a string of characters platform \u2192 hardware \u2192 cpu \u2192 number of cores Description: Number of CPU cores. Type: Scalar ; usually an integer number platform \u2192 hardware \u2192 gpu \u2192 architecture Description: Name of the GPU architecture. Type: Scalar ; usually a string of characters platform \u2192 hardware \u2192 gpu \u2192 number of cores Description: Number of GPU cores. Type: Scalar ; usually an integer number platform \u2192 hardware \u2192 gpu \u2192 memory Description: Amount of available memory of the GPU; string with numbers followed by GB. Type: Scalar ; usually a string of characters platform \u2192 hardware \u2192 ram Description: Amount of available RAM; string with numbers followed by GB. Type: Scalar ; usually a string of characters platform \u2192 operating system \u2192 kernel Description: The kernel version of the operating system. Type: Scalar ; usually a string of characters platform \u2192 operating system \u2192 distribution Description: The name of the operating system's distribution. Type: Scalar ; usually a string of characters platform \u2192 software \u2192 libraries Description: Names and versions of the software libraries and packages underlying the experiment's Implementation. Type: Sequence of scalars for each programming language; string representing the name of the software library and version if available platform \u2192 software \u2192 retrieval toolkit Description: Names and versions of the retrieval toolkits underlying the experiment's Implementation . Type: Sequence of scalars ; string representing the name of the retrieval toolkit and version if available Example platform: hardware: cpu: model: Intel(R) Xeon(R) Gold 6144 CPU @ 3.50GHz architecture: x86_64 operation mode: 64-bit number of cores: 16 gpu: model: NVIDIA RTX A6000 memory: 48 GB number of cores: 10752 ram: 32 GB operating system: kernel: GNU/Linux 4.15.0-166-generic distribution: Ubuntu 18.04.5 LTS software: libraries: python: - blas==1.0 - libgfortran==3.0.1 - libxml2==2.9.8 - lightgbm==2.2.1 - ncurses==6.1 - numpy==1.15.4 - numpy-base==1.15.4 - scikit-learn==0.20.1 - scipy==1.1.0 - setuptools==40.6.2 java: - lucene==7.6 retrieval toolkit: - anserini==0.3.0","title":"Platform"},{"location":"metadata/platform/#platform","text":"The metadata information about the Platform should include the underlying hardware, the operating system, and the used software libraries. software covers every package or library that is used for the Implementation . If a retrieval toolkit is used, it should be reported explicitly.","title":"Platform"},{"location":"metadata/platform/#checklist","text":"platform \u2192 hardware \u2192 cpu \u2192 model Description: Name of the CPU model. Type: Scalar ; usually a string of characters platform \u2192 hardware \u2192 cpu \u2192 architecture Description: Identifier of the CPU architecture. Type: Scalar ; usually a string of characters platform \u2192 hardware \u2192 cpu \u2192 operation mode Description: Operation mode of the CPU. Type: Scalar ; usually a string of characters platform \u2192 hardware \u2192 cpu \u2192 number of cores Description: Number of CPU cores. Type: Scalar ; usually an integer number platform \u2192 hardware \u2192 gpu \u2192 architecture Description: Name of the GPU architecture. Type: Scalar ; usually a string of characters platform \u2192 hardware \u2192 gpu \u2192 number of cores Description: Number of GPU cores. Type: Scalar ; usually an integer number platform \u2192 hardware \u2192 gpu \u2192 memory Description: Amount of available memory of the GPU; string with numbers followed by GB. Type: Scalar ; usually a string of characters platform \u2192 hardware \u2192 ram Description: Amount of available RAM; string with numbers followed by GB. Type: Scalar ; usually a string of characters platform \u2192 operating system \u2192 kernel Description: The kernel version of the operating system. Type: Scalar ; usually a string of characters platform \u2192 operating system \u2192 distribution Description: The name of the operating system's distribution. Type: Scalar ; usually a string of characters platform \u2192 software \u2192 libraries Description: Names and versions of the software libraries and packages underlying the experiment's Implementation. Type: Sequence of scalars for each programming language; string representing the name of the software library and version if available platform \u2192 software \u2192 retrieval toolkit Description: Names and versions of the retrieval toolkits underlying the experiment's Implementation . Type: Sequence of scalars ; string representing the name of the retrieval toolkit and version if available","title":"Checklist"},{"location":"metadata/platform/#example","text":"platform: hardware: cpu: model: Intel(R) Xeon(R) Gold 6144 CPU @ 3.50GHz architecture: x86_64 operation mode: 64-bit number of cores: 16 gpu: model: NVIDIA RTX A6000 memory: 48 GB number of cores: 10752 ram: 32 GB operating system: kernel: GNU/Linux 4.15.0-166-generic distribution: Ubuntu 18.04.5 LTS software: libraries: python: - blas==1.0 - libgfortran==3.0.1 - libxml2==2.9.8 - lightgbm==2.2.1 - ncurses==6.1 - numpy==1.15.4 - numpy-base==1.15.4 - scikit-learn==0.20.1 - scipy==1.1.0 - setuptools==40.6.2 java: - lucene==7.6 retrieval toolkit: - anserini==0.3.0","title":"Example"},{"location":"metadata/research_goal/","text":"Research goal The metadata information about the Research goal should include information about the venue for which the study was made, the corresponding publications, as well as some information about the evaluation. If the Actor is reported as reproducer , the baseline refers to the tag of the original run that is reimplemented, otherwise, it should be a strong and reasonable baseline if the Actor is the original experimenter . Checklist research goal \u2192 venue \u2192 name Description: Name of the venue (e.g. journal or conference) at which is the study is published. Type: Scalar ; usually a string of characters research goal \u2192 venue \u2192 year Description: Year in which the study was published. Type: Scalar ; usually an integer number research goal \u2192 publication \u2192 dblp Description: URL of the publication in the dblp - computer science bibliography . Type: Scalar ; usually a string of characters research goal \u2192 publication \u2192 doi Description: DOI of the publication. Type: Scalar ; usually a string of characters research goal \u2192 publication \u2192 arxiv Description: URL to the arXiv publication. Type: Scalar ; usually a string of characters research goal \u2192 publication \u2192 url Description: Custom URL where is the publication is hosted. Type: Scalar ; usually a string of characters research goal \u2192 publication \u2192 abstract Description: Abstract of the publication. Type: Scalar ; usually a string of characters research goal \u2192 evaluation \u2192 reported_measures Description: A list of measures that were evaluated. We propose to follow trec_eval 's naming convention of the measures. Type: Sequence of scalars ; usually a listing of strings research goal \u2192 evaluation \u2192 baseline Description: The run tag of the baseline that is used in the experiments. If the Actor is the original experimenter , the baseline should be adequate and state-of-the-art. If the Actor is a reproducer the baseline refers to the run that is reproduced. Type: Sequence of scalars ; usually a listing of strings research goal \u2192 evaluation \u2192 significance test Description: Significance tests that were used as part of the experimental evaluations. If required the corresponding correction method should be reported as well. Type: Sequence of mappings ; a single mapping usually contains a scalar of the name and correction method Example research goal: venue: name: ECIR year: 2019 publication: dblp: https://dblp.org/rec/conf/ecir/YuXL19 doi: https://doi.org/10.1007/978-3-030-15712-8_26 url: https://cs.uwaterloo.ca/~jimmylin/publications/Yu_etal_ECIR2019.pdf abstract: We tackle the problem of transferring relevance judgments across document collections for specific information needs by reproducing and generalizing the work of Grossman and Cormack from the TREC 2017 Common Core Track. Their approach involves training relevance classifiers using human judgments on one or more existing (source) document collections and then applying those classifiers to a new (target) document collection. Evaluation results show that their approach, based on logistic regression using word-level tf-idf features, is both simple and effective, with average precision scores close to human-in-the-loop runs. The original approach required inference on every document in the target collection, which we reformulated into a more efficient reranking architecture using widely-available open-source tools. Our efforts to reproduce the TREC results were successful, and additional experiments demonstrate that relevance judgments can be effectively transferred across collections in different combinations. We affirm that this approach to cross-collection relevance feedback is simple, robust, and effective. evaluation: reported measures: - map - P_10 baseline: - WCrobust04 significance test: - name: 't-test' correction method: 'bonferroni'","title":"Research Goal"},{"location":"metadata/research_goal/#research-goal","text":"The metadata information about the Research goal should include information about the venue for which the study was made, the corresponding publications, as well as some information about the evaluation. If the Actor is reported as reproducer , the baseline refers to the tag of the original run that is reimplemented, otherwise, it should be a strong and reasonable baseline if the Actor is the original experimenter .","title":"Research goal"},{"location":"metadata/research_goal/#checklist","text":"research goal \u2192 venue \u2192 name Description: Name of the venue (e.g. journal or conference) at which is the study is published. Type: Scalar ; usually a string of characters research goal \u2192 venue \u2192 year Description: Year in which the study was published. Type: Scalar ; usually an integer number research goal \u2192 publication \u2192 dblp Description: URL of the publication in the dblp - computer science bibliography . Type: Scalar ; usually a string of characters research goal \u2192 publication \u2192 doi Description: DOI of the publication. Type: Scalar ; usually a string of characters research goal \u2192 publication \u2192 arxiv Description: URL to the arXiv publication. Type: Scalar ; usually a string of characters research goal \u2192 publication \u2192 url Description: Custom URL where is the publication is hosted. Type: Scalar ; usually a string of characters research goal \u2192 publication \u2192 abstract Description: Abstract of the publication. Type: Scalar ; usually a string of characters research goal \u2192 evaluation \u2192 reported_measures Description: A list of measures that were evaluated. We propose to follow trec_eval 's naming convention of the measures. Type: Sequence of scalars ; usually a listing of strings research goal \u2192 evaluation \u2192 baseline Description: The run tag of the baseline that is used in the experiments. If the Actor is the original experimenter , the baseline should be adequate and state-of-the-art. If the Actor is a reproducer the baseline refers to the run that is reproduced. Type: Sequence of scalars ; usually a listing of strings research goal \u2192 evaluation \u2192 significance test Description: Significance tests that were used as part of the experimental evaluations. If required the corresponding correction method should be reported as well. Type: Sequence of mappings ; a single mapping usually contains a scalar of the name and correction method","title":"Checklist"},{"location":"metadata/research_goal/#example","text":"research goal: venue: name: ECIR year: 2019 publication: dblp: https://dblp.org/rec/conf/ecir/YuXL19 doi: https://doi.org/10.1007/978-3-030-15712-8_26 url: https://cs.uwaterloo.ca/~jimmylin/publications/Yu_etal_ECIR2019.pdf abstract: We tackle the problem of transferring relevance judgments across document collections for specific information needs by reproducing and generalizing the work of Grossman and Cormack from the TREC 2017 Common Core Track. Their approach involves training relevance classifiers using human judgments on one or more existing (source) document collections and then applying those classifiers to a new (target) document collection. Evaluation results show that their approach, based on logistic regression using word-level tf-idf features, is both simple and effective, with average precision scores close to human-in-the-loop runs. The original approach required inference on every document in the target collection, which we reformulated into a more efficient reranking architecture using widely-available open-source tools. Our efforts to reproduce the TREC results were successful, and additional experiments demonstrate that relevance judgments can be effectively transferred across collections in different combinations. We affirm that this approach to cross-collection relevance feedback is simple, robust, and effective. evaluation: reported measures: - map - P_10 baseline: - WCrobust04 significance test: - name: 't-test' correction method: 'bonferroni'","title":"Example"}]}